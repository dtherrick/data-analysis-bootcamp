{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:755792ea31ded83414e3caaa6127c419d265c57404dcaf3d2999ca50855988ba"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Part 3: ANALYTICS: Modeling and Making Predictions with your data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Agenda:\n",
      "* Making Guesses\n",
      "    * Applying a framework to our guesses\n",
      "* Finding Clusters\n",
      "    * K-Means Clustering Example\n",
      "* Finding Attributes / Dimensionality Reduction\n",
      "    * Workshop: Principal Components Analysis\n",
      "* Predictive Modeling\n",
      "    * Get to know scikit-learn\n",
      "* Data Management\n",
      "    * Loading Data\n",
      "    * Normalizing Data\n",
      "    * Standardizing Data\n",
      "    * Imputing Data\n",
      "* Predictive Modeling Examples:\n",
      "    * Supervised Learning\n",
      "    * Ensemble Methods\n",
      "    * Advanced"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "###Making Guesses:\n",
      "* Developing the capacity to approximate an answer is a crucial skill for any analyst / data scientist\n",
      "* Beginning our section on analytics with a discussion on guesstimation is the best place to start.\n",
      "\n",
      "####Why start with guesses?\n",
      "* They're intuitive\n",
      "* They help you find an approximate answer\n",
      "* They are simple enough to explain\n",
      "* It connects a __conceptual__ understanding with the __concrete reality__ of the problem domain\n",
      "* They leave you with no place to hide.\n",
      "* The generate actual __numbers__\n",
      "\n",
      ">The point of this: find an approximate answer - quickly, easily, and cheaply. But it also forces us to think about the accuracy of our guess, and how to communicate our understanding."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "###Principles of guesstimation:\n",
      "* Estimate __sizes__ of things by comparing them to something you know.\n",
      "* Establish __functional relationships__ by using simplifying assumptions\n",
      "* Errors - even small ones - can compound dramatically. Track the __accuracy__ of your estimate closely.\n",
      "* Bad guesses on things that you aren't familiar with can have a disastrous effect; but you can easily correct that as better information is available.\n",
      "\n",
      "####When guessing, we'd like to be within an order of magnitude of the actual answer."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "###Estimating Sizes:\n",
      "* Compare the unknown with something you know.\n",
      "    * Example: soldiers in the Middle Ages who wanted to scale a castle wall. How to determine the height of the ladder?\n",
      "        * Too short: can't make the top of the wall.\n",
      "        * Too tall: overhang the wall, and the occupants of the castle can tip you over.\n",
      "    * The soldiers counted the the rows of stone blocks used in the fortress and used that as a guide for height.\n",
      "* How to get good at this?\n",
      "    * Learn the sizes of things around you. Be an observer.\n",
      "    * Look things up\n",
      "        * Generate your own lookup tables to keep nearby for reference.\n",
      "  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "###Establishing Relationships:\n",
      "* Business situations: typically somewhat simple (how many items fit into someting else, etc)\n",
      "\n",
      ">For example, how many boxes fit inside a semi trailer?\n",
      "> A mathematician may start thinking about packing theory, and how to optimize the boxes.\n",
      "> But if you want a simple estimate: divide the volume of a trailer by the volume of a typical package.\n",
      "\n",
      "* This raises an important - maybe the most important - point about guesstimates:\n",
      "> The point is to retain the core of the problem, stripping away as much nonessential detail as possible. Don't let sophistication get in the way of simple answers."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "###Working with Numbers:\n",
      "* __Don't go for the calculator first!!__\n",
      "* It can interrupt the creative process, and change the way you think about the problem.\n",
      "* There are techniques that work for back of the envelope calculations:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "####Powers of ten:\n",
      "* A simple tool for multiplying large numbers:\n",
      "    * Do this equation in your head: $$\\frac{9,000 \\centerdot 17}{400}$$\n",
      "    * Break it down:\n",
      "        $$9,000 = 9 \\centerdot 1,000$$\n",
      "        \n",
      "        $$17 \\approxeq 20 = 2 \\centerdot 10$$\n",
      "        \n",
      "        $$400 = 4 \\centerdot 100$$\n",
      "\n",
      "     * So we can do this:\n",
      "        $$\\frac{9 \\centerdot 2}{4} = 4.5$$\n",
      "     \n",
      "        $$1,000 = 10^3; 10 = 10^1; 100 = 100^2$$\n",
      "\n",
      "        $$\\frac{1000 \\centerdot 10}{100} = \\frac{10^{(3+1)}}{10^2} = 10^{(3+1-2)} = 10^2 = 100$$\n",
      "        \n",
      "        $$4.5 \\centerdot 100 = 450$$\n",
      "    \n",
      "    * Our estimate is __450__. The actual answer is __382.5__. Not bad.\n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Finding Clusters\n",
      "* __Cluster analysis__ is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). \n",
      "\n",
      "* It is a main task of exploratory data mining, and a common technique for statistical data analysis \n",
      "* Clustering is used in many fields, including \n",
      "    * Machine Learning \n",
      "    * Pattern Recognition\n",
      "    * Image Analysis\n",
      "    * Information Retrieval\n",
      "    * Bioinformatics\n",
      "\n",
      "* Cluster analysis describes the general task to be solved. \n",
      "* It can be achieved by various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them. \n",
      "    * Popular notions of clusters include:\n",
      "        * Groups with small distances among the cluster members, \n",
      "        * Dense areas of the data space \n",
      "        * Particular statistical distributions. \n",
      "* The appropriate clustering algorithm and parameter settings depend on the individual data set and intended use of the results. \n",
      "* Cluster analysis is therefore an iterative process that involves a great deal of trial and failure before a satisfying result is achieved. \n",
      "* It will often be necessary to modify data preprocessing and model parameters until the result achieves the desired properties."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Clustering Algorithms\n",
      "* The notion of a \"cluster\" cannot be precisely defined, leading to many clustering algorithms. \n",
      "\n",
      "* There is a common denominator: a group of data objects, but the notion of a cluster, varies significantly in its properties. \n",
      "\n",
      "* Understanding these \"cluster models\" is key to understanding the differences between the various algorithms. \n",
      "    * __It is also beyond the scope of this course__\n",
      "\n",
      "* Typical cluster models include:\n",
      "    * __Connectivity models__: for example hierarchical clustering builds models based on distance connectivity.\n",
      "    * __Centroid models__: for example the k-means algorithm represents each cluster by a single mean vector.\n",
      "    * __Distribution models__: clusters are modeled using statistical distributions, such as multivariate normal distributions used by the Expectation-maximization algorithm.\n",
      "    * __Density models__: for example DBSCAN and OPTICS defines clusters as connected dense regions in the data space.\n",
      "    * __Subspace models__: in Biclustering (also known as Co-clustering or two-mode-clustering), clusters are modeled with both cluster members and relevant attributes.\n",
      "    * __Group models__: some algorithms do not provide a refined model for their results and just provide the grouping information.\n",
      "    * __Graph-based models__: a clique, i.e., a subset of nodes in a graph such that every two nodes in the subset are connected by an edge can be considered as a prototypical form of cluster. \n",
      "\n",
      "* For more information about clustering, read the Wikipedia article on [Cluster Analysis](https://en.wikipedia.org/wiki/Cluster_analysis)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##K-means Clustering\n",
      "* Intuitive technique that involves defining a set number of clusters on the front of the model, then optimizing to determine the correct number based on the minimum distance of each point from the center of the cluster.\n",
      "* Computationally expensive"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "###Mathematical Description\n",
      "* Given a set of observations $(x_1, x_2, \\ldots, x_n)$, where each observation is a d-dimensional real vector, k-means clustering aims to partition the n observations into $k (\\leq n)$ sets $S: \\{S_1, S_2 \\ldots, S_k\\}$ so as to minimize the within-cluster sum of squares (WCSS). \n",
      "\n",
      "* In other words, its objective is to find:\n",
      "\n",
      "$$\\underset{\\mathbf{S}} {\\operatorname{arg\\,min}}  \\sum_{i=1}^{k} \\sum_{\\mathbf x \\in S_i} \\left\\| \\mathbf x - \\boldsymbol\\mu_i \\right\\|^2$$\n",
      "\n",
      "where $\u03bc_i$ is the mean of points in $S_i$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Algorithm\n",
      "* The k-means algorithm uses an iterative refinement technique.\n",
      "* Given an initial set of k means $m_1(1) \\ldots ,m_k$ (see below), the algorithm proceeds by alternating between two steps:\n",
      "    * __Assignment step__: Assign each observation to the cluster whose mean yields the least within-cluster sum of squares (WCSS). Since the sum of squares is the squared Euclidean distance, this is intuitively the \"nearest\" mean.\n",
      "        * Each $x_p$ is assigned to exactly one $S^{(t)}$\n",
      "        * $$S_i^{(t)} = \\big \\{ x_p : \\big \\| x_p - m^{(t)}_i \\big \\|^2 \\le \\big \\| x_p - m^{(t)}_j \\big \\|^2 \\ \\forall j, 1 \\le j \\le k \\big\\}$$\n",
      "    * __Update step__: Calculate the new means to be the centroids of the observations in the new clusters\n",
      "$$m^{(t+1)}_i = \\frac{1}{|S^{(t)}_i|} \\sum_{x_j \\in S^{(t)}_i} x_j$$ \n",
      "Since the arithmetic mean is a least-squares estimator, this also minimizes the within-cluster sum of squares (WCSS) objective."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##K-Means Clustering Example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Finding Attributes / Dimensionality Reduction\n",
      "* The process of reducing the number of random variables under consideration. \n",
      "* Can be divided into \n",
      "    * Feature selection\n",
      "    * Feature extraction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "##Workshop: Principal Components Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Getting to know scikit-learn\n",
      "* Scikit-learn provides a range of supervised and unsupervised learning algorithms via a consistent interface in Python. \n",
      "* The library is built upon the SciPy stack (Scientific Python) that must be installed before you can use scikit-learn. This is stack that includes:\n",
      "    * __NumPy__: Base n-dimensional array package\n",
      "    * __SciPy__: Fundamental library for scientific computing\n",
      "    * __Matplotlib__: Comprehensive 2D/3D plotting\n",
      "    * __IPython__: Enhanced interactive console\n",
      "    * __Sympy__: Symbolic mathematics\n",
      "    * __Pandas__: Data structures and analysis\n",
      "* Extensions or modules for SciPy are conventionally named SciKits. \n",
      "    * Since this module provides learning algorithms it is named __scikit-learn__."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##What are the features?\n",
      "* The library is focused on modeling data. \n",
      "* It is not focused on loading, manipulating and summarizing data. \n",
      "    * For these features, refer to NumPy and Pandas.\n",
      "\n",
      "* Some groups of models provided by scikit-learn include:\n",
      "    * __Clustering__: for grouping unlabeled data such as KMeans.\n",
      "    * __Cross Validation__: for estimating the performance of supervised models on unseen data.\n",
      "    * __Datasets__: for test datasets and for generating datasets with specific properties for investigating model behaviour.\n",
      "    * __Dimensionality Reduction__: for reducing the number of attributes in data for summarization, visualization and feature selection.\n",
      "    * __Ensemble methods__: for combining the predictions of multiple supervised models.\n",
      "    * __Feature extraction__: for defining attributes in image and text data.\n",
      "    * __Feature selection__: for identifying meaningful attributes from which to create supervised models.\n",
      "    * __Parameter Tuning__: for getting the most out of supervised models.\n",
      "    * __Manifold Learning__: for summarizing and depicting complex multi-dimensional data.\n",
      "    * __Supervised Models__: a vast array not limited to generalized linear models, discriminant analysis, naive bayes, lazy methods, neural networks, support vector machines and decision trees."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Example Datasets\n",
      "* The scikit-learn library is packaged with five datasets. These datasets are useful for getting a handle on a given machine learning algorithm or library feature before using it in your own work."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load and display details of the packaged datasets\n",
      "from sklearn.datasets import load_boston\n",
      "from sklearn.datasets import load_iris\n",
      "from sklearn.datasets import load_diabetes\n",
      "from sklearn.datasets import load_digits\n",
      "from sklearn.datasets import load_linnerud"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Library/Python/2.7/site-packages/numpy/core/fromnumeric.py:2499: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
        "  VisibleDeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Boston house prices dataset (13x506, reals, regression)\n",
      "boston = load_boston()\n",
      "print(boston.data.shape)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(506, 13)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Iris flower dataset (4x150, reals, multi-label classification)\n",
      "iris = load_iris()\n",
      "print(iris.data.shape)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(150, 4)\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Diabetes dataset (10x442, reals, regression)\n",
      "diabetes = load_diabetes()\n",
      "print(diabetes.data.shape)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(442, 10)\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Hand-written digit dataset (64x1797, multi-label classification)\n",
      "digits = load_digits()\n",
      "print(digits.data.shape)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1797, 64)\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Linnerud psychological and exercise dataset (3x20,3x20 multivariate regression)\n",
      "linnerud = load_linnerud()\n",
      "print(linnerud.data.shape)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(20, 3)\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Load Data From CSV\n",
      "* CSV files are an extremely common tool to distribute data.\n",
      "    * They can live on your local workstation or on a remote server. \n",
      "    \n",
      "* Demonstration: \n",
      "    * Load a CSV file from a URL, \n",
      "        * The Pima Indians diabetes classification dataset from the UCI Machine Learning Repository. \n",
      "    * From the prepared X and y variables, you can train a machine learning model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import urllib\n",
      "\n",
      "url = \"http://goo.gl/j0Rvxq\"\n",
      "\n",
      "raw_data = urllib.urlopen(url)\n",
      "\n",
      "dataset = np.loadtxt(raw_data, delimiter=\",\")\n",
      "print(dataset.shape)\n",
      "\n",
      "X = dataset[:, 0:7]\n",
      "Y = dataset[:, 8]\n",
      "\n",
      "dataset[1:3,:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(768, 9)\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "array([[   1.   ,   85.   ,   66.   ,   29.   ,    0.   ,   26.6  ,\n",
        "           0.351,   31.   ,    0.   ],\n",
        "       [   8.   ,  183.   ,   64.   ,    0.   ,    0.   ,   23.3  ,\n",
        "           0.672,   32.   ,    1.   ]])"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Normalization\n",
      "* Normalization refers to rescaling real valued numeric attributes into the range 0 and 1. \n",
      "* It is useful to scale the input attributes for a model that relies on the magnitude of values:\n",
      "    * distance measures used in k-nearest neighbors\n",
      "    * preparation of coefficients in regression."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_iris\n",
      "from sklearn import preprocessing\n",
      "\n",
      "iris = load_iris()\n",
      "print(iris.data.shape)\n",
      "\n",
      "X = iris.data\n",
      "y = iris.target\n",
      "\n",
      "normalized_X = preprocessing.normalize(X)\n",
      "print X[0:5]\n",
      "print normalized_X[0:5]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(150, 4)\n",
        "[[ 5.1  3.5  1.4  0.2]\n",
        " [ 4.9  3.   1.4  0.2]\n",
        " [ 4.7  3.2  1.3  0.2]\n",
        " [ 4.6  3.1  1.5  0.2]\n",
        " [ 5.   3.6  1.4  0.2]]\n",
        "[[ 0.80377277  0.55160877  0.22064351  0.0315205 ]\n",
        " [ 0.82813287  0.50702013  0.23660939  0.03380134]\n",
        " [ 0.80533308  0.54831188  0.2227517   0.03426949]\n",
        " [ 0.80003025  0.53915082  0.26087943  0.03478392]\n",
        " [ 0.790965    0.5694948   0.2214702   0.0316386 ]]\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Standardization\n",
      "* Standardization refers to shifting the distribution of each attribute to have a mean of 0 and a standard deviation of 1. \n",
      "    * It is useful to standardize attributes for a model that relies on the distribution of attributes such as Gaussian processes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_iris\n",
      "from sklearn import preprocessing\n",
      "\n",
      "iris = load_iris()\n",
      "print(iris.data.shape)\n",
      "\n",
      "X = iris.data\n",
      "y = iris.target\n",
      "\n",
      "standardized_X = preprocessing.scale(X)\n",
      "print X[0:5]\n",
      "print standardized_X[0:5]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(150, 4)\n",
        "[[ 5.1  3.5  1.4  0.2]\n",
        " [ 4.9  3.   1.4  0.2]\n",
        " [ 4.7  3.2  1.3  0.2]\n",
        " [ 4.6  3.1  1.5  0.2]\n",
        " [ 5.   3.6  1.4  0.2]]\n",
        "[[-0.90068117  1.03205722 -1.3412724  -1.31297673]\n",
        " [-1.14301691 -0.1249576  -1.3412724  -1.31297673]\n",
        " [-1.38535265  0.33784833 -1.39813811 -1.31297673]\n",
        " [-1.50652052  0.10644536 -1.2844067  -1.31297673]\n",
        " [-1.02184904  1.26346019 -1.3412724  -1.31297673]]\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Imputing\n",
      "* Data can have missing values. \n",
      "* These are values for attributes where:\n",
      "    * a measurement could not be taken\n",
      "    * Data is corrupted for an unknown reason. \n",
      "* Important to identify and mark this missing data. \n",
      "* Prepare replacement values. \n",
      "    * This is useful because some algorithms are unable to work with or exploit missing data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import urllib\n",
      "from sklearn.preprocessing import Imputer\n",
      "\n",
      "url = \"http://goo.gl/j0Rvxq\"\n",
      "\n",
      "raw_data = urllib.urlopen(url)\n",
      "\n",
      "dataset = np.loadtxt(raw_data, delimiter=\",\")\n",
      "print(dataset.shape)\n",
      "\n",
      "X = dataset[:, 0:7]\n",
      "Y = dataset[:, 8]\n",
      "\n",
      "X[X==0] = np.nan\n",
      "\n",
      "imp = Imputer(missing_values='NaN', strategy='mean')\n",
      "imputed_X = imp.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(768, 9)\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Predictive Modeling Examples:\n",
      "* Supervised Learning\n",
      "    * Linear Regression\n",
      "    * Logistic Regression\n",
      "    * Naive Bayes\n",
      "    * Support Vector Machines (SVM)\n",
      "        * Classification\n",
      "        * Prediction\n",
      "* Ensemble Methods\n",
      "    * Random Forest\n",
      "        * Classification\n",
      "        * Prediction\n",
      "* Advanced\n",
      "    * Feature Importance\n",
      "    * Cross Validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Linear Regression\n",
      "* Linear regression fits a linear model (such as a line in two dimensions) to the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn import datasets\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "dataset = datasets.load_diabetes()\n",
      "\n",
      "model = LinearRegression()\n",
      "model.fit(dataset.data, dataset.target)\n",
      "print(model)\n",
      "\n",
      "expected = dataset.target\n",
      "predicted = model.predict(dataset.data)\n",
      "\n",
      "mse = np.mean((predicted - expected)**2)\n",
      "print \"MSE = \" + str(mse)\n",
      "print(model.score(dataset.data, dataset.target))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LinearRegression(copy_X=True, fit_intercept=True, normalize=False)\n",
        "MSE = 2859.69039877\n",
        "0.517749425413\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Logistic Regression\n",
      "* Logistic regression fits a logistic model to data and makes predictions about the probability of an event (between 0 and 1).\n",
      "    * Fit a logistic regression algorithm to the iris dataset. \n",
      "    * Because this is a mutliclass classification problem and logistic regression makes predictions between 0 and 1, a one-vs-all scheme is used (one model is created per class)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "from sklearn import metrics\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "dataset = datasets.load_iris()\n",
      "\n",
      "model = LogisticRegression()\n",
      "model.fit(dataset.data, dataset.target)\n",
      "print(model)\n",
      "\n",
      "expected = dataset.target\n",
      "predicted = model.predict(dataset.data)\n",
      "\n",
      "print(metrics.classification_report(expected, predicted))\n",
      "print(metrics.confusion_matrix(expected, predicted))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "          intercept_scaling=1, penalty=l2, random_state=None, tol=0.0001)\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       1.00      1.00      1.00        50\n",
        "          1       0.98      0.90      0.94        50\n",
        "          2       0.91      0.98      0.94        50\n",
        "\n",
        "avg / total       0.96      0.96      0.96       150\n",
        "\n",
        "[[50  0  0]\n",
        " [ 0 45  5]\n",
        " [ 0  1 49]]\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Naive Bayes\n",
      "* Naive Bayes uses Bayes Theorem to model the conditional relationship of each attribute to the class variable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "from sklearn import metrics\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "\n",
      "dataset = datasets.load_iris()\n",
      "\n",
      "model = GaussianNB()\n",
      "model.fit(dataset.data, dataset.target)\n",
      "print(model)\n",
      "\n",
      "expected = dataset.target\n",
      "predicted = model.predict(dataset.data)\n",
      "\n",
      "#summarize the fit\n",
      "print(metrics.classification_report(expected, predicted))\n",
      "print(metrics.confusion_matrix(expected, predicted))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "GaussianNB()\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       1.00      1.00      1.00        50\n",
        "          1       0.94      0.94      0.94        50\n",
        "          2       0.94      0.94      0.94        50\n",
        "\n",
        "avg / total       0.96      0.96      0.96       150\n",
        "\n",
        "[[50  0  0]\n",
        " [ 0 47  3]\n",
        " [ 0  3 47]]\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Support Vector Machines\n",
      "* Support Vector Machines (SVM) are a method that uses points in a transformed problem space that best separate classes into two groups. \n",
      "* Classification for multiple classes is supported by a one-vs-all method. \n",
      "* SVM also supports regression by modeling the function with a minimum amount of allowable error."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "##SVM - Classification\n",
      "* iris dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "from sklearn import metrics\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "dataset = datasets.load_iris()\n",
      "\n",
      "model = SVC()\n",
      "model.fit(dataset.data, dataset.target)\n",
      "print(model)\n",
      "\n",
      "expected = dataset.target\n",
      "predicted = model.predict(dataset.data)\n",
      "\n",
      "print metrics.classification_report(expected, predicted)\n",
      "print metrics.confusion_matrix(expected, predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
        "  kernel=rbf, max_iter=-1, probability=False, random_state=None,\n",
        "  shrinking=True, tol=0.001, verbose=False)\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       1.00      1.00      1.00        50\n",
        "          1       1.00      0.96      0.98        50\n",
        "          2       0.96      1.00      0.98        50\n",
        "\n",
        "avg / total       0.99      0.99      0.99       150\n",
        "\n",
        "[[50  0  0]\n",
        " [ 0 48  2]\n",
        " [ 0  0 50]]\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "##SVM - Prediction\n",
      "* diabetes dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn import datasets\n",
      "from sklearn.svm import SVR\n",
      "\n",
      "dataset = datasets.load_diabetes()\n",
      "\n",
      "model = SVR()\n",
      "model.fit(dataset.data, dataset.target)\n",
      "print(model)\n",
      "\n",
      "expected = dataset.target\n",
      "predicted = model.predict(dataset.data)\n",
      "\n",
      "mse = np.mean((predicted - expected)**2)\n",
      "print \"MSE = \" + str(mse)\n",
      "print model.score(dataset.data, dataset.target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.0,\n",
        "  kernel=rbf, max_iter=-1, probability=False, random_state=None,\n",
        "  shrinking=True, tol=0.001, verbose=False)\n",
        "MSE = 6024.46500728\n",
        "-0.0159497379817\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Random Forest\n",
      "* The Random Forest ensemble method creates a number of decision trees, \n",
      "    * Each tree created on a different subset of the dataset. \n",
      "    * Generally the approach to creating an ensemble used by random forest is called bootstrap aggregation\n",
      "        * Bagging for short."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "##Random Forest - Classification\n",
      "* Classifications for the iris dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "from sklearn import metrics\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "dataset = datasets.load_iris()\n",
      "\n",
      "model = RandomForestClassifier()\n",
      "model.fit(dataset.data, dataset.target)\n",
      "print(model)\n",
      "\n",
      "expected = dataset.target\n",
      "predicted = model.predict(dataset.data)\n",
      "\n",
      "print metrics.classification_report(expected, predicted)\n",
      "print metrics.confusion_matrix(expected, predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RandomForestClassifier(bootstrap=True, compute_importances=None,\n",
        "            criterion=gini, max_depth=None, max_features=auto,\n",
        "            min_density=None, min_samples_leaf=1, min_samples_split=2,\n",
        "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
        "            verbose=0)\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       1.00      1.00      1.00        50\n",
        "          1       1.00      1.00      1.00        50\n",
        "          2       1.00      1.00      1.00        50\n",
        "\n",
        "avg / total       1.00      1.00      1.00       150\n",
        "\n",
        "[[50  0  0]\n",
        " [ 0 50  0]\n",
        " [ 0  0 50]]\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "##Random Forest - Prediction\n",
      "* Predictions on the diabetes dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn import datasets\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "\n",
      "dataset = datasets.load_diabetes()\n",
      "\n",
      "model = RandomForestRegressor()\n",
      "model.fit(dataset.data, dataset.target)\n",
      "print(model)\n",
      "\n",
      "expected = dataset.target\n",
      "predicted = model.predict(dataset.data)\n",
      "\n",
      "mse = np.mean((predicted-expected)**2)\n",
      "\n",
      "print \"MSE = \" + str(mse)\n",
      "print model.score(dataset.data, dataset.target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RandomForestRegressor(bootstrap=True, compute_importances=None, criterion=mse,\n",
        "           max_depth=None, max_features=auto, min_density=None,\n",
        "           min_samples_leaf=1, min_samples_split=2, n_estimators=10,\n",
        "           n_jobs=1, oob_score=False, random_state=None, verbose=0)\n",
        "MSE = 617.067330317\n",
        "0.89593940843\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Recursive Feature Elimination\n",
      "* The Recursive Feature Elimination (RFE) method is a feature selection approach. \n",
      "    * It works by recursively removing attributes and building a model on those attributes that remain. \n",
      "    * It uses the model accuracy to identify which attributes / combination of attributes contribute the most to predicting the target attribute.\n",
      "\n",
      "* Demonstarate the use of RFE on the iris dataset to select 3 attributes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "from sklearn.feature_selection import RFE\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "dataset = datasets.load_iris()\n",
      "model = LogisticRegression()\n",
      "\n",
      "rfe = RFE(model, 3)\n",
      "rfe = rfe.fit(dataset.data, dataset.target)\n",
      "\n",
      "print rfe.support_\n",
      "print rfe.ranking_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[False  True  True  True]\n",
        "[2 1 1 1]\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Feature Importance\n",
      "* Methods that use ensembles of decision trees (like Random Forest and Extra Trees) can also compute the relative importance of each attribute. \n",
      "    * These importance values can be used to inform a feature selection process.\n",
      "\n",
      "* Demonstrate the construction of an Random Forest ensemble of the iris dataset and the display of the relative feature importance."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "from sklearn import metrics\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "\n",
      "dataset = datasets.load_iris()\n",
      "\n",
      "model = ExtraTreesClassifier()\n",
      "model.fit(dataset.data, dataset.target)\n",
      "\n",
      "print model.feature_importances_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.15613814  0.0326185   0.27375869  0.53748466]\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Cross Validation\n",
      "* Cross validation is a way of evaluating a model on a dataset. \n",
      "* It provides an estimation of the accuracy of the model if it were to make predictions on previously unseen data. \n",
      "* Cross validation estimations are used to aid in the selection of a robust model that is fit for purpose.\n",
      "\n",
      "* This recipe estimates the performance of logistic regression on the iris dataset using k-fold cross validation with 10 folds."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn import datasets\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import cross_validation\n",
      "\n",
      "dataset = datasets.load_iris()\n",
      "\n",
      "#prepare cross validation folds\n",
      "num_folds = 10\n",
      "num_instances = len(dataset.data)\n",
      "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds)\n",
      "\n",
      "model = LogisticRegression()\n",
      "\n",
      "# evaluate the model k-fold cross validation\n",
      "results = cross_validation.cross_val_score(model, dataset.data, dataset.target, cv=kfold)\n",
      "\n",
      "# this shows mean classification accuracy on each fold\n",
      "print results\n",
      "\n",
      "# display the mean and stdev of the classification accuracy\n",
      "print results.mean()\n",
      "print results.std()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 1.          1.          1.          0.86666667  0.73333333  0.66666667\n",
        "  1.          0.93333333  0.6         1.        ]\n",
        "0.88\n",
        "0.148473716342\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}